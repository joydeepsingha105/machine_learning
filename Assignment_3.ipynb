{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. List down the step of Model development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps of machine learning are the following : \n",
    "\n",
    "1. The collection of the data set considering the quantity and the quality. \n",
    "2. Preparation of the data through expoloratory data analysis where we remove missing values, perform normalisation, remove duplicates etc. For a supervised learning we divide the data into train and test data. \n",
    "3. Choose the appropriate machine learnining algorithm. \n",
    "4. Train the model using the subset of data chosen from the main data set. \n",
    "5. Evaluation of the fitting of the train data using the chosen madel using the metrics, eg. precision, recall score, accuracy etc. Using these metrics we also check for overfitting and underfitting. \n",
    "6. Examination of the model using the test data set and check the values of different metrics to evaluate the performance of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define R square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While developing a model from a given dataset, the goodness of fit of the model or the $R^2$ is defined as the variance of the response variable that are predictable from the independent variable.\n",
    "\n",
    "Let us consider $y_{i}, i \\in (1, 2, \\cdots n)$ as the actual data of a certain $n$ values of a response variable $y$ and $\\widetilde{y}_{i}, i \\in (1, 2, \\cdots n)$ are the values obtained after the fitting the model. Then $R^2$ is defined as, \n",
    "\\begin{equation}\n",
    "R^{2} = \\frac{\\sum\\limits_{i = 1}^{n}(\\widetilde{y}_{i} - \\bar{y})^{2}}{\\sum\\limits_{i = 1}^{n}(y_{i} -  \\bar{y})^{2}}\n",
    "\\end{equation}\n",
    "where $\\bar{y} = \\frac{1}{n}\\sum\\limits_{i = 1}^{n}y_{i}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define Adjusted R square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For given model, $R^{2}$ increases as the number of parameters are introduced in it. Also if the model has too many higher order terms then due to over fitting of the data the value of $R^2$ increases and results in misleading prediction. It is for this reason, $R^2$ must be modified include the number of parameters. \n",
    "\n",
    "Recalling the definition of $R^2$, of the $n$ response variables and $k$ number of parameters then the adjusted $R_{\\text{adj}}^{2}$ is given as following, \n",
    "\\begin{equation}\n",
    "R_{\\text{adj}}^{2} = 1 - (1 - R^{2})\\frac{n - 1}{n - k - 1}\n",
    "\\end{equation}\n",
    "The advantage of $R_{\\text{adj}}^{2}$ is that due to the presence of $k$, increasing the number of parameter does not increase its value as oppose to $R^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define mean square error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During a model developement, let us consider $y_{i}, i \\in (1, 2, \\cdots n)$ as the actual data of a certain $n$ values of a response variable $y$ and $\\widetilde{y}_{i}, i \\in (1, 2, \\cdots n)$ are the values obtained from the model. Then the Mean Squared Error (MSE) is defined as, \n",
    "\\begin{equation}\n",
    "\\text{MSE} = \\frac{1}{n}\\sum\\limits_{i = 1}^{n}(y_{i} - \\widetilde{y}_{i})^{2}\n",
    "\\end{equation}\n",
    "\n",
    "While building a model one of the goal is to reduce the MSE error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Define root mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Squared Error (RMSE) is defined as the squared root of MSE. \n",
    "\\begin{equation}\n",
    "\\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{n}\\sum\\limits_{i = 1}^{n}(y_{i} - \\widetilde{y}_{i})^{2}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Define Mean absolute percentage error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us consider $y_{i}, i \\in (1, 2, \\cdots n)$ as the actual data of a certain $n$ values of a response variable $y$ and $\\widetilde{y}_{i}, i \\in (1, 2, \\cdots n)$ are the values obtained from the model. Then the mean absolute percentage error is defined as the following, \n",
    "\\begin{equation}\n",
    "\\text{MAPE} = \\frac{100}{n}\\sum\\limits_{i = 1}^{n}\\left|\\frac{y_{i} - \\widetilde{y}_{i}}{y_{i}}\\right|\n",
    "\\end{equation}\n",
    "Thus is an unsigned percentage error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. What are the assumptions of linear regression ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumptions of linear regression process are, \n",
    "\n",
    "1. The first assumption is that there is a linear relationship between the independent variables and targets. \n",
    "2. The indenpendent variables are not inter-correlated. \n",
    "3. The distribution of the error (residuals) between an independent variable and the target should be same accross all the independent variables. \n",
    "4. The distribution of the residuals follow a gaussian distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Define Multiconlinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While model building, multicollinearity refers to the situation when the independent variable are intercorrelated. The existence of such multicollinearity increases the error in the coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Make a Wine price prediction model through Linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year   Price  WinterRain     AGST  HarvestRain  Age  FrancePop\n",
      "0  1952  7.4950         600  17.1167          160   31  43183.569\n",
      "1  1953  8.0393         690  16.7333           80   30  43495.030\n",
      "2  1955  7.6858         502  17.1500          130   28  44217.857\n",
      "3  1957  6.9845         420  16.1333          110   26  45152.252\n",
      "4  1958  6.7772         582  16.4167          187   25  45653.805\n",
      "(25, 7)\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "wine = pd.read_csv('wine.csv')\n",
    "print(wine.head())\n",
    "print(wine.shape)\n",
    "print(len(wine['Price']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for the goodness of Fit : \n",
      "\n",
      "R square value :  0.9540351801833349\n",
      "Adjusted R square value :  0.9387135735777798\n",
      "\n",
      "Errors in prediction : \n",
      "\n",
      "Mean_Squared_Error : 0.28081588058085105\n",
      "Root_Mean_Squared_Error : 0.5299206361153065\n",
      "Mean_Absolute_Percentage_Error : 0.4643290276631289\n"
     ]
    }
   ],
   "source": [
    "X = wine[['WinterRain', 'HarvestRain', 'FrancePop', 'Age', 'AGST']]\n",
    "y = wine[['Price']]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state = 100)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "my_model = LinearRegression()\n",
    "\n",
    "########## Fitting the model ########\n",
    "my_model.fit(X_train, y_train)\n",
    "count_row = X_train.shape[0]\n",
    "count_column = X.shape[1]\n",
    "y_pred = my_model.predict(X_train)\n",
    "df = pd.concat((pd.DataFrame(y_pred), pd.DataFrame(y_train)),axis =1)\n",
    "from sklearn.metrics import r2_score\n",
    "adj_r2 = 1.0 - ((1.0 - r2_score(y_train, y_pred))*(count_row - 1.0)/(count_row - (count_column - 1.0) - 1.0))\n",
    "\n",
    "print('Parameters for the goodness of Fit : \\n')\n",
    "print('R square value : ', r2_score(y_train, y_pred))\n",
    "print('Adjusted R square value : ', adj_r2)\n",
    "\n",
    "########## Prediction ###########\n",
    "y_pred = my_model.predict(X_test)\n",
    "df = pd.concat((pd.DataFrame(y_pred), pd.DataFrame(y_test)),axis =1)\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "print('\\nErrors in prediction : \\n')\n",
    "print('Mean_Squared_Error :' ,mean_squared_error(y_test, y_pred))\n",
    "print('Root_Mean_Squared_Error :' ,sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print('Mean_Absolute_Percentage_Error :' , mean_absolute_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the table attached with this notebook, we find that the linear regression model considering all the independent variables, 'WinterRain', 'AGST', 'HarvestRain', 'Age', 'FrancePop' results in a highest $R^2$ and $R^{2}_{adj}$ value indicating best fit of the model. However if we consider the values of MSE, RMSE and MAPE for the test data then the model consisting of 'WinterRain', 'AGST' and 'HarvestRain' result in high value of $R^2$ and $R^{2}_{\\text{adj}}$ as well as low values of MSE, RMSE and MAPE. In case a model with less independent variables is preferred than the later linear regression model with the three attributes 'WinterRain', 'AGST' and 'HarvestRain' can be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. How do decision trees work ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees is a machine learning tool for predictions and have application in a variety of areas. As the name suggests, a decision tree is am upside down tree-like graph structure with nodes and edges. Here the nodes represents the place where we pick a an attribute and pose a if-else type question. In this way a decision tree sorts the data starting from the root to the leaf nodes. Each node in the tree acts as a condition for an attribute and each edge emerging from the attribute corresponds one of the probable answer to the test case. This is a recursive process and is repeated for the subsequent nodes till a certain condition on the content of the node is satisfied. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Define Splitting and stopping criteria of Decision Tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The splitting and stopping criteria for a decision tree is indicated by two parameters 'minbucket' and 'minsplit'. \n",
    "\n",
    "'minbucket' denotes the minimum number of samples that must be in the leaf nodes after the splitting. If the samples in the leaf nodes are less than 'minbucket' then the split of the node is stopped. \n",
    "\n",
    "In case of a splitting criteria if the number of samples in a node is less than 'minsplit' then the split of such node is not allowed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. What is entropy ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of a decision tree, let a target variable have two catagories of values. From the data set, if we calculate the probabilites for the two class of values of the target as $p$ and $q$ then the entropy is defined as, \n",
    "\\begin{equation}\n",
    "E(p, q) = -p\\log p - q\\log q\n",
    "\\end{equation}\n",
    "The value of the entropy denotes order of randomness in the target variable. Now if $X_i, i = 1, 2, \\cdots, n$ are the predictors and $T$ is the target variable then after the decision tree is made the entropy of the target variables in the final leaf nodes are \n",
    "\\begin{equation}\n",
    "E(T, X) = \\sum\\limits_{c \\in X}P(c)E(c) \n",
    "\\end{equation}\n",
    "where $c$ is classes of each attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. What is Gini?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gini index is defined as the sum of square of the probabilities of the target variable for each classes of the attributes subtracted from one. If $P_{i}$ probabilities for each attribute then the Gini is given by, \n",
    "\\begin{equation}\n",
    "\\text{GINI} = 1 - \\sum\\limits_{i = 1}^{c}P_{i}^{2}\n",
    "\\end{equation}\n",
    "Similar to the definition of Entropy, the Gini index for the end leaf nodes in a decision tree is calculated by multiplying the class probibilities with Gini index of each classes and summing them. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. What is information gain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information gain refers to decrease in the value of Entropy after more attributes are taken into account to construct the decision trees. If the Entropy of the target variables $E(T)$ are $E(T)$ and $E(T, X)$ is Entropy after consider all $X$ attributes and its classes then the information gain is $E(p, q) - E(T, X)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. How to identify overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify overfitting, one can use cross validation method. To calculate this we divide the test data in $k$ segments. Then the data in one segment is used as a test data set and remaining $k - 1$ segment is used as a training data set for the model to fit and estimation errors (eg. $R^2$) are calculated. This process is repeated for the entire data set. The average esitmated error for all of the $k$ trials are then averaged and compared to errors for model fitted to the entire data set. The existence of a large difference between the two errors thus calculated will indicate an overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
